# EncoderTransformerArchitecture (BERT)
Building Transformers from scratch for regression and classification tasks. The modules include:

Multi-head Attention

Transformer Block(s)

Positional Encoding

Encoder / Decoder
