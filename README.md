# EncoderTransformerArchitecture (BERT)
Building Transformers from scratch for both regression and classification tasks. The modules include:

Multi-head Attention

Transformer Block(s)

Positional Encoding

Encoder / Decoder
